<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Open-LLM-VTuber</title>

    <!-- pixi live2d dependencies -->
    <!-- Load Cubism and PixiJS -->
    <!-- <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script> -->
    <script src="libs/live2dcubismcore.min.js"></script>
    <!-- <script src="https://cdn.jsdelivr.net/gh/dylanNew/live2d/webgl/Live2D/lib/live2d.min.js"></script> -->
    <script src="libs/live2d.min.js"></script>
    <!-- <script src="https://cdn.jsdelivr.net/npm/pixi.js@7.x/dist/pixi.min.js"></script> -->
    <script src="libs/pixi.min.js"></script>

    <!-- <script src="https://cdn.jsdelivr.net/gh/RaSan147/pixi-live2d-display@v0.5.0-ls-7/dist/index.min.js"></script> -->
    <script src="libs/index.min.js"></script>



    <script src="TaskQueue.js"></script>

    <!-- Voice Activation Detection -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script> -->
    <script src="libs/ort.js"></script>
    <!-- <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script> -->
    <script src="libs/bundle.min.js"></script>

    <link rel="stylesheet" href="index.css">
</head>

<body>
    <div class="top-left">
        <button id="wsStatus">Disconnected</button>
        <button id="stateDisplay">Status: loading</button>
        <button id="pointerInteractionBtn">ğŸ‘€ Pointer Interactive On</button>
        <input type="text" id="wsUrl" placeholder="WebSocket URL">
        <div class="sensitivity-container">
            <span class="sensitivity-label">Speech Prob. Threshold:</span>
            <input type="number" id="speechProbThreshold" min="1" max="100" value="99" title="Speech Detection Confidence Level Threshold (%)">
        </div>
        <select id="configDropdown" aria-label="Configuration Selection">
            <option value="">Select Configuration</option>
        </select>
        <select id="bgDropdown" aria-label="Background Selection">
            <option value="">Select Background</option>
        </select>
    </div>

    <canvas id="canvas"></canvas>

    <div class="bottom-container">
        <div class="fixed-bottom" id="message"></div>
        <div class="control-buttons">
            <button id="micToggle">ğŸ™ï¸Mic is On</button>
            <button id="interruptBtn">âŒVoice Interruption Off</button>
        </div>
    </div>

    <!-- <script src="./modelDict.js"></script> -->
    <script src="./live2d.js"></script>

    <script>
        // idle: When the LLM is not thinking or speaking and is waiting for user input.
        // thinking-speaking: When the LLM is thinking or speaking.
        // interrupted: When the LLM is interrupted by the user.
        let state = "idle"; // idle, thinking-speaking, interrupted
        let audioPlayer = new Audio();
        let voiceInterruptionOn = false;
        let fullResponse = ""; // full response from the server in one conversation chain

        const stateDisplay = document.getElementById('stateDisplay');

        function updateStateDisplay() {
            stateDisplay.textContent = `Status: ${state}`;
        }

        function setState(newState) {
            state = newState;
            updateStateDisplay();
        }

        function interrupt() {
            console.log("ğŸ˜¡ğŸ‘ Interrupting conversation chain");
            console.log("Sending: " + JSON.stringify({ type: "interrupt-signal", text: fullResponse }))
            ws.send(JSON.stringify({ type: "interrupt-signal", text: fullResponse }));
            setState("interrupted");
            model2.stopSpeaking();
            audioTaskQueue.clearQueue();
            console.log("Interrupted!!!!");
        }

        let myvad;
        let previousTriggeredProbability = 0; // the possibility that triggered the last speech start
        let speechProbThreshold = document.getElementById('speechProbThreshold');

        async function init_vad() {
            myvad = await vad.MicVAD.new({
                preSpeechPadFrames: 20,
                positiveSpeechThreshold: speechProbThreshold.value / 100,
                onSpeechStart: () => {
                    console.log("Speech start detected: " + previousTriggeredProbability);
                    if (state === "thinking-speaking") {
                        interrupt();
                    } else {
                        console.log("ğŸ˜€ğŸ‘ Not interrupted. Just normal conversation");
                    }
                },
                onFrameProcessed: (probs) => {
                    if (probs["isSpeech"] > previousTriggeredProbability) {
                        previousTriggeredProbability = probs["isSpeech"];
                    }
                },
                onVADMisfire: () => {
                    console.log("VAD Misfire. The LLM can't hear you.");
                    if (state === "interrupted") {
                        state = "idle";
                    }
                    document.getElementById("message").textContent = "The LLM can't hear you.";
                },
                onSpeechEnd: (audio) => {
                    // audio: (Float32Array of audio samples at sample rate 16000)...
                    // make sure the audio queue is empty before getting new ones
                    audioTaskQueue.clearQueue();

                    if (!voiceInterruptionOn) {
                        stop_mic();
                    }

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        sendAudioPartition(audio);
                    }
                }
            });
        }

        speechProbThreshold.addEventListener('change', async function() {
            if (myvad) {
                await myvad.pause();
                await init_vad();
                if (micToggleState) {
                    await myvad.start();
                }
            }
        });

        const chunkSize = 4096;
        async function sendAudioPartition(audio) {
            console.log(audio)
            // send the audio, a Float32Array of audio samples at sample rate 16000, to the back end by chunks
            for (let index = 0; index < audio.length; index += chunkSize) {
                const endIndex = Math.min(index + chunkSize, audio.length);
                const chunk = audio.slice(index, endIndex);
                ws.send(JSON.stringify({ type: "mic-audio-data", audio: chunk }));
            }
            ws.send(JSON.stringify({ type: "mic-audio-end" }));
        }

        // window.addEventListener('load', init_vad);

        // WebSocket connection
        let ws;
        const wsStatus = document.getElementById('wsStatus');
        const wsUrl = document.getElementById('wsUrl');
        const interruptBtn = document.getElementById('interruptBtn');
        const micToggle = document.getElementById('micToggle');
        const configDropdown = document.getElementById('configDropdown');
        const bgDropdown = document.getElementById('bgDropdown');

        wsUrl.value = "ws://127.0.0.1:12393/client-ws";
        // if running on server
        if (window.location.protocol.startsWith("http")) {
            console.log("Running on server");
            wsUrl.value = "/client-ws";
        } else { // if running on local using file://
            console.log("Running on local");
        }

        function connectWebSocket() {
            ws = new WebSocket(wsUrl.value);

            ws.onopen = function () {
                // interrupted = false;
                setState("idle");
                console.log("Connected to WebSocket");
                wsStatus.textContent = "Connected";
                wsStatus.classList.add('connected');
                fetchConfigurations();
                fetchBackgrounds();
            };

            ws.onclose = function () {
                // interrupt = false;
                setState("idle");
                console.log("Disconnected from WebSocket");
                wsStatus.textContent = "Disconnected";
                wsStatus.classList.remove('connected');
                taskQueue.clearQueue();
            };

            ws.onmessage = function (event) {
                handleMessage(JSON.parse(event.data));
            };
        }

        wsStatus.addEventListener('click', connectWebSocket);

        function handleMessage(message) {
            console.log("Received Request: \n", message);
            switch (message.type) {
                case "full-text":
                    document.getElementById("message").textContent = message.text;
                    console.log(message);
                    console.log("full-text: ", message.text);
                    break;
                case "control":
                    switch (message.text) {
                        case "start-mic":
                            start_mic();
                            break;
                        case "stop-mic":
                            stop_mic();
                            break;
                        case "conversation-chain-start":
                            setState("thinking-speaking");
                            fullResponse = "";
                            break;
                        case "conversation-chain-end":
                            setState("idle");
                            if (!voiceInterruptionOn) {
                                start_mic();
                            }
                            break;
                    }
                    break;
                case "expression":
                    setExpression(message.text);
                    break;
                case "mouth":
                    setMouth(Number(message.text));
                    break;
                case "audio":
                    if (state == "interrupted") {
                        console.log("Audio playback intercepted. Sentence:", message.text);
                    } else {
                        addAudioTask(message.audio, message.volumes, message.slice_length, message.text, message.expressions);
                        // playAudioLipSync(message.audio, message.volumes, message.slice_length, message.text, message.expressions);
                    }
                    break;
                case "set-model":
                    console.log("set-model: ", message.text);
                    live2dModule.init().then(() => {
                        live2dModule.loadModel(message.text);
                    });
                    break;
                case "listExpressions":
                    console.log(listSupportedExpressions());
                    break;
                case "config-files":
                    populateConfigDropdown(message.files);
                    break;
                case "config-switched":
                    console.log(message.message);
                    document.getElementById("message").textContent = "Configuration switched successfully!";
                    setState("idle");

                    // restore the mic state before switching config
                    if (micStateBeforeConfigSwitch) {
                        start_mic();
                    }
                    micStateBeforeConfigSwitch = null;  // reset the state
                    break;
                case "background-files":
                    populateBgDropdown(message.files);
                    break;
                default:
                    console.error("Unknown message type: " + message.type);
                    console.log(message);
            }
        }

        function fetchConfigurations() {
            ws.send(JSON.stringify({ type: "fetch-configs" }));
        }

        function fetchBackgrounds() {
            ws.send(JSON.stringify({ type: "fetch-backgrounds" }));
        }

        function populateConfigDropdown(files) {
            configDropdown.innerHTML = '<option value="">Select Configuration</option>';
            files.forEach(file => {
                const option = document.createElement('option');
                option.value = file;
                option.textContent = file;
                configDropdown.appendChild(option);
            });
        }

        function populateBgDropdown(files) {
            bgDropdown.innerHTML = '<option value="">Select Background</option>';
            files.forEach(file => {
                const option = document.createElement('option');
                option.value = file;
                option.textContent = file;
                bgDropdown.appendChild(option);
            });
        }

        configDropdown.addEventListener('change', function () {
            const selectedConfig = configDropdown.value;
            if (selectedConfig) {
                setState("switching-config");
                document.getElementById("message").textContent = "Switching configuration...";
                // avoid the mic being on when switching config
                micStateBeforeConfigSwitch = micToggleState;
                if (micToggleState) {
                    stop_mic();
                }

                interrupt();
                ws.send(JSON.stringify({ type: "switch-config", file: selectedConfig }));
            }
        });

        bgDropdown.addEventListener('change', function () {
            const selectedBg = bgDropdown.value;
            if (selectedBg) {
                document.body.style.backgroundImage = `url('./bg/${selectedBg}')`;
            }
        });

        // set expression of the model2
        // @param {int} expressionIndex - the expression index defined in the emotionMap in modelDict.js
        function setExpression(expressionIndex) {
            expressionIndex = parseInt(expressionIndex);
            model2.internalModel.motionManager.expressionManager.setExpression(expressionIndex);
            console.info(`>> [x] -> Expression set to: (${expressionIndex})`);
        }

        // [Deprecated] Check if the string contains an expression. If it does, set the expression of the model2.
        // @param {string} str - the string to check
        function checkStringForExpression(str) {
            console.log("emo map: ", emoMap);
            for (const key of Object.keys(emoMap)) {
                if (str.toLowerCase().includes("[" + key + "]")) {
                    console.info(">> [ ] <- add to exec queue: " + key + ", " + emoMap[key]);
                    taskQueue.addTask(() => { setExpression(emoMap[key]); });
                    taskQueue.addTask(() => { console.log("timing out..."); });
                    // setExpression(emoMap[key]);
                }
            }
        }
        // [Deprecated] List all supported expressions
        function listSupportedExpressions() {
            emoMap = model2.internalModel.motionManager.expressionManager.emotionMap;
            console.log(emoMap);
        }



        function setMouth(mouthY) {
            if (typeof model2.internalModel.coreModel.setParameterValueById === 'function') {
                model2.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', mouthY);
            } else {
                model2.internalModel.coreModel.setParamFloat('PARAM_MOUTH_OPEN_Y', mouthY);
            }
        }

        audioTaskQueue = new TaskQueue(100); // 100ms delay between tasks
        async function addAudioTask(audio_base64, volumes, slice_length, text = null, expression_list = null) {
            console.log(`1. Adding audio task ${text} to queue`);

            // calculate the total duration of the audio
            audioLength = await getAudioLength(audio_base64);
            console.log(`2. Audio length: ${audioLength}`);

            audioTaskQueue.addTask(async () => {
                playAudioLipSync(audio_base64, volumes, slice_length, text, expression_list);
                await new Promise(resolve => setTimeout(resolve, audioLength));
                console.log(`3. Audio task ${text} completed`);
            });
        }

        async function getAudioLength(audio_base64) {
            return new Promise((resolve) => {
                const audio = new Audio("data:audio/wav;base64," + audio_base64);
                audio.onloadedmetadata = () => {
                    const audioDur = audio.duration * 1000;
                    resolve(audioDur);
                };
            });
        }
            // å…¨å±€å˜é‡å£°æ˜
        let virtualMouseInterval = null;
        let lastMouseX = window.innerWidth / 2;
        let lastMouseY = window.innerHeight / 2;
        let currentX = lastMouseX;
        let currentY = lastMouseY;
        let targetX = lastMouseX;
        let targetY = lastMouseY;

        // æ‰©å±•é…ç½®å¯¹è±¡
        const modelMovementConfig = {
            enabled: false,
            headPosition: 0.0, // å¤´éƒ¨ä½ç½®ç³»æ•°
            transitioning: false,   
            
            // æ°´å¹³æ–¹å‘é…ç½®
            radiusX: 0.3,        // æ°´å¹³æ™ƒåŠ¨èŒƒå›´
            offsetX: 0,           // æ°´å¹³åç§»(-1åˆ°1ï¼Œè´Ÿå€¼åå·¦ï¼Œæ­£å€¼åå³)
            rangeMinX: -0.5,        // æ°´å¹³èŒƒå›´æœ€å°å€¼ï¼ˆç›¸å¯¹ä¸­å¿ƒï¼‰
            rangeMaxX: 0.5,         // æ°´å¹³èŒƒå›´æœ€å¤§å€¼ï¼ˆç›¸å¯¹ä¸­å¿ƒï¼‰
            updateIntervalX: 2500,// æ°´å¹³æ–¹å‘æ›´æ–°é—´éš”(ms)
            
            // å‚ç›´æ–¹å‘é…ç½®
            radiusY: 0.75,        // å‚ç›´æ™ƒåŠ¨èŒƒå›´
            offsetY: 0,           // å‚ç›´åç§»(-1åˆ°1ï¼Œè´Ÿå€¼åä¸Šï¼Œæ­£å€¼åä¸‹)
            rangeMinY: 0.1,        // å‚ç›´èŒƒå›´æœ€å°å€¼ï¼ˆç›¸å¯¹ä¸­å¿ƒï¼‰
            rangeMaxY: 1,         // å‚ç›´èŒƒå›´æœ€å¤§å€¼ï¼ˆç›¸å¯¹ä¸­å¿ƒï¼‰
            updateIntervalY: 500,// å‚ç›´æ–¹å‘æ›´æ–°é—´éš”(ms)
            
            // é€šç”¨é…ç½®
            interval: 16,         // æ¸²æŸ“æ›´æ–°é¢‘ç‡(ms)
            smoothness: 0.09,      // å¹³æ»‘ç³»æ•°(0-1)
            debug: false
        };

        const centerPosition = {
            x: window.innerWidth / 2,
            y: window.innerHeight / 2
        };

        // ä¸Šæ¬¡æ›´æ–°æ—¶é—´è®°å½•
        let lastUpdateX = 0;
        let lastUpdateY = 0;

        function lerp(start, end, t) {
            return start + (end - start) * t;
        }

        // è·å–å¸¦èŒƒå›´é™åˆ¶çš„éšæœºå€¼
        function getRandomInRange(min, max, offset = 0, radius = 1) {
            // å°†-1åˆ°1çš„offsetæ˜ å°„åˆ°å®é™…èŒƒå›´
            const center = (max + min) / 2 + (max - min) / 2 * offset;
            const actualMin = Math.max(min, center - radius * (max - min) / 2);
            const actualMax = Math.min(max, center + radius * (max - min) / 2);
            return actualMin + Math.random() * (actualMax - actualMin);
        }

        function getModelCenter() {
            if (!model2) return { x: window.innerWidth / 2, y: window.innerHeight / 2 };
            
            const modelX = model2.position.x + model2.width / 2;
            const modelY = model2.position.y + model2.height * modelMovementConfig.headPosition;
            
            if(modelMovementConfig.debug) {
                console.log('Model center:', { x: modelX, y: modelY });
            }
            
            return { x: modelX, y: modelY };
        }

        function clearMovement() {
            if(virtualMouseInterval) {
                clearInterval(virtualMouseInterval);
                virtualMouseInterval = null;
            }
        }

        window.modelControl = {
            start: function() {
                if(modelMovementConfig.enabled || modelMovementConfig.transitioning) {
                    if(modelMovementConfig.debug) {
                        console.log('Model movement is already running or transitioning');
                    }
                    return;
                }
                
                clearMovement();
                modelMovementConfig.enabled = true;
                
                const updatePosition = () => {
                    if(!model2 || !modelMovementConfig.enabled) {
                        this.stop();
                        return;
                    }
                    
                    const now = Date.now();
                    const modelCenter = getModelCenter();
                    
                    // æ›´æ–°æ°´å¹³ä½ç½®
                    if (now - lastUpdateX > modelMovementConfig.updateIntervalX) {
                        const xRange = model2.width * modelMovementConfig.radiusX;
                        const normalizedX = getRandomInRange(
                            modelMovementConfig.rangeMinX,
                            modelMovementConfig.rangeMaxX,
                            modelMovementConfig.offsetX,
                            1
                        );
                        targetX = modelCenter.x + normalizedX * xRange;
                        lastUpdateX = now;
                        
                        if(modelMovementConfig.debug) {
                            console.log('New X target:', {
                                offset: normalizedX,
                                absolute: targetX - modelCenter.x
                            });
                        }
                    }
                    
                    // æ›´æ–°å‚ç›´ä½ç½®
                    if (now - lastUpdateY > modelMovementConfig.updateIntervalY) {
                        const yRange = model2.height * modelMovementConfig.radiusY;
                        const normalizedY = getRandomInRange(
                            modelMovementConfig.rangeMinY,
                            modelMovementConfig.rangeMaxY,
                            modelMovementConfig.offsetY,
                            1
                        );
                        targetY = modelCenter.y + normalizedY * yRange;
                        lastUpdateY = now;
                        
                        if(modelMovementConfig.debug) {
                            console.log('New Y target:', {
                                offset: normalizedY,
                                absolute: targetY - modelCenter.y
                            });
                        }
                    }
                    
                    // å¹³æ»‘æ’å€¼åˆ°ç›®æ ‡ä½ç½®
                    currentX = lerp(currentX, targetX, modelMovementConfig.smoothness);
                    currentY = lerp(currentY, targetY, modelMovementConfig.smoothness);
                    
                    if(model2 && model2.internalModel) {
                        model2.focus(currentX, currentY);
                    }
                };
                
                virtualMouseInterval = setInterval(updatePosition, modelMovementConfig.interval);
                
                console.log('Model movement started');
                this.status();
            },
            
            stop: function() {
                if(!modelMovementConfig.enabled) {
                    if(modelMovementConfig.debug) {
                        console.log('Model movement is already stopped or transitioning');
                    }
                    return;
                }
                
                modelMovementConfig.enabled = false;
                modelMovementConfig.transitioning = true;  // æ ‡è®°æ­£åœ¨è¿‡æ¸¡
                
                // è®¾ç½®ç›®æ ‡ä¸ºä¸­å¿ƒç‚¹
                targetX = centerPosition.x;
                targetY = centerPosition.y;
                
                // åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„intervalæ¥å¤„ç†å¹³æ»‘è¿‡æ¸¡
                const transitionInterval = setInterval(() => {
                    const transitionSmoothness = modelMovementConfig.smoothness * 0.5;
                    
                    currentX = lerp(currentX, targetX, transitionSmoothness);
                    currentY = lerp(currentY, targetY, transitionSmoothness);
                    
                    if(model2 && model2.internalModel) {
                        model2.focus(currentX, currentY);
                    }
                    
                    // å½“è¶³å¤Ÿæ¥è¿‘ç›®æ ‡ç‚¹æ—¶åœæ­¢è¿‡æ¸¡
                    const distanceX = Math.abs(currentX - targetX);
                    const distanceY = Math.abs(currentY - targetY);
                    if(distanceX < 0.1 && distanceY < 0.1) {
                        clearInterval(transitionInterval);
                        clearInterval(virtualMouseInterval);
                        virtualMouseInterval = null;
                        modelMovementConfig.transitioning = false;  // è¿‡æ¸¡ç»“æŸ
                        
                        // æœ€åä¸€æ¬¡è®¾ç½®åˆ°ç²¾ç¡®çš„ä¸­å¿ƒç‚¹
                        if(model2 && model2.internalModel) {
                            model2.focus(centerPosition.x, centerPosition.y);
                        }
                        
                        if(modelMovementConfig.debug) {
                            console.log('Transition completed, model centered');
                        }
                    }
                }, modelMovementConfig.interval);
                
                if(modelMovementConfig.debug) {
                    console.log('Model movement stopping with smooth transition');
                }
            },
            
            set: function(params) {
                const oldConfig = {...modelMovementConfig};
                
                Object.keys(params).forEach(key => {
                    if(modelMovementConfig.hasOwnProperty(key)) {
                        modelMovementConfig[key] = params[key];
                    }
                });
                
                if(modelMovementConfig.enabled) {
                    this.stop();
                    this.start();
                }
                
                console.log('Config updated:', {
                    old: oldConfig,
                    new: {...modelMovementConfig}
                });
            },
            
            debug: function() {
                modelMovementConfig.debug = !modelMovementConfig.debug;
                console.log('Debug mode:', modelMovementConfig.debug ? 'on' : 'off');
            },
            
            status: function() {
                console.log('Current status:', {...modelMovementConfig});
            }
        };


        
        // ä¿®æ”¹playAudioLipSyncå‡½æ•°ï¼Œä½¿ç”¨modelControlæ¥æ§åˆ¶
        function playAudioLipSync(audio_base64, volumes, slice_length, text = null, expression_list = null) {
            if (state === "interrupted") {
                console.error("Audio playback blocked. Sentence:", text);
                audioTaskQueue.clearQueue();
                return;
            }

            fullResponse += text;
            if (text) {
                document.getElementById("message").textContent = text;
            }

            // ç­‰å¾…ä»»ä½•ç°æœ‰çš„è¿åŠ¨æˆ–è¿‡æ¸¡å®Œæˆ
            const startNewMovement = () => {
                if(modelMovementConfig.transitioning) {
                    setTimeout(startNewMovement, 100);
                    return;
                }

                modelControl.set({
                    updateIntervalX: 2400,  // å¾ˆæ…¢çš„æ°´å¹³æ›´æ–°
                    updateIntervalY: 750    // å¿«é€Ÿçš„å‚ç›´æ›´æ–°
                });
                modelControl.start();
            };

            // å¦‚æœå½“å‰æœ‰è¿åŠ¨ï¼Œå…ˆåœæ­¢
            if(modelMovementConfig.enabled) {
                modelControl.stop();
            }
            
            // ç­‰å¾…çŸ­æš‚å»¶è¿Ÿåå¼€å§‹æ–°çš„è¿åŠ¨
            setTimeout(startNewMovement, 100);

            const displayExpression = expression_list ? expression_list[0] : null;
            console.log("Start playing audio: ", text);
            
            model2.speak("data:audio/wav;base64," + audio_base64, { 
                expression: displayExpression, 
                resetExpression: false,
                onFinish: () => {
                    modelControl.stop();
                }
            });
        }

        // Start the microphone. This will start the VAD and send audio to the server when speech is detected.
        // Once speech ends, the mic will pause.
        async function start_mic() {
            if (myvad == null) await init_vad();
            console.log("Mic start ");
            myvad.start();
            micToggleState = true;
            micToggle.textContent = "ğŸ™ï¸Mic is On";
        }

        function stop_mic() {
            console.log("Mic stop");
            myvad.pause();
            micToggleState = false;
            micToggle.textContent = "âŒMic is off";
        }

        interruptBtn.addEventListener('click', function () {
            voiceInterruptionOn = !voiceInterruptionOn;
            interruptBtn.textContent = voiceInterruptionOn ? "ğŸ–ï¸Voice Interruption On" : "âŒVoice Interruption Off";
        });

        let micToggleState = true;
        micToggle.addEventListener('click', function () {
            micToggleState ? stop_mic() : start_mic();
        });

        // Initialize WebSocket connection
        connectWebSocket();
        document.addEventListener('mousemove', (e) => {
        // åªåœ¨éè¯´è¯çŠ¶æ€ä¸‹æ›´æ–°æœ€åçš„é¼ æ ‡ä½ç½®
            if(state !== "thinking-speaking") {
                lastMouseX = e.clientX;
                lastMouseY = e.clientY;
                
                // åªåœ¨æœªå¯ç”¨è‡ªåŠ¨ç§»åŠ¨ä¸”éè¯´è¯çŠ¶æ€æ—¶å“åº”é¼ æ ‡
                if(!modelMovementConfig.enabled && model2 && model2.internalModel) {
                    currentX = lastMouseX;
                    currentY = lastMouseY;
                    model2.focus(lastMouseX, lastMouseY);
                }
            }
        });
    </script>
</body>

</html>
